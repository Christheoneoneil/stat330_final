{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09.00-maj-causal-inference-direct-effects\n",
    "\n",
    "In this notebook we revisit our causal inference model once again.\n",
    "Using the online toolkit from Daggity, we can measure both the total effect and direct effect of\n",
    "\n",
    "- SAT scores on acceptance\n",
    "- Race on acceptance\n",
    "- Gender on acceptance\n",
    "<br></br>\n",
    "- Total effect of SAT: $Y \\sim SAT + GPA + Income$\n",
    "- Direct effect of SAT: $Y \\sim SAT + Gender + Race + School$\n",
    "- Total effect of Gender/Race: $Y \\sim Gender + GPA + SAT + Race + School$\n",
    "- Direct effect of Gender/Race: $Y \\sim Gender/Race$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from psis import *\n",
    "\n",
    "import stan\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_code = \"\"\"\n",
    "/**\n",
    " * Logistic regression t-prior\n",
    " *\n",
    " * Priors:\n",
    " *     weights - student t\n",
    " *     intercept - student t\n",
    " */\n",
    "data {\n",
    "    int n;                        // number of data points\n",
    "    int d;                        // explanatory variable dimension\n",
    "    matrix[n, d] X;               // explanatory variable\n",
    "    array[n] int <lower=0, upper=1> y;  // response variable\n",
    "    int<lower=1> p_alpha_df;      // prior degrees of freedom for alpha\n",
    "    real p_alpha_loc;             // prior location for alpha\n",
    "    real p_alpha_scale;           // prior scale for alpha\n",
    "    int<lower=1> p_beta_df;       // prior degrees of freedom for beta\n",
    "    real p_beta_loc;     // prior location for beta\n",
    "    real p_beta_scale;            // prior scale for beta\n",
    "\n",
    "    int<lower=0> N_new;\n",
    "    matrix[N_new, d] X_new;\n",
    "}\n",
    "parameters {\n",
    "    real alpha;      // intercept\n",
    "    vector[d] beta;  // explanatory variable weights\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[n] eta;  // linear predictor\n",
    "    eta = alpha + X * beta;\n",
    "}\n",
    "model {\n",
    "    alpha ~ student_t(p_alpha_df, p_alpha_loc, p_alpha_scale);\n",
    "    beta ~ student_t(p_beta_df, p_beta_loc, p_beta_scale);\n",
    "    y ~ bernoulli_logit(eta);\n",
    "}\n",
    "generated quantities {\n",
    "    // calculate the log likelihood\n",
    "    vector[n] log_lik;\n",
    "    for (i in 1:n)\n",
    "        log_lik[i] = bernoulli_logit_lpmf(y[i] | eta[i]);\n",
    "\n",
    "    // generate values for Y tilde\n",
    "    array[n] int<lower=0, upper=1> Y_tilde;\n",
    "    Y_tilde = bernoulli_logit_rng(eta);\n",
    "\n",
    "    // Make predictions\n",
    "    vector[N_new] y_new;\n",
    "    for (j in 1:N_new) {\n",
    "        y_new[j] = bernoulli_logit_rng(alpha + X_new[j] * beta);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data_dict):\n",
    "    logistic_regression = stan.build(logistic_regression_code, data=data_dict)\n",
    "    fit = logistic_regression.sample()\n",
    "    return fit\n",
    "\n",
    "def get_summary_table(fit, X_train_df, col=None):\n",
    "    az_fit = az.from_pystan(fit)\n",
    "    summary_table = az.summary(az_fit, var_names=['alpha', 'beta'])\n",
    "    index_labels = [\"intercept\"] + list(X_train_df.columns)\n",
    "    summary_table.index = index_labels\n",
    "    if col:\n",
    "        return summary_table.loc[col, :]\n",
    "    else:\n",
    "        return summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJID</th>\n",
       "      <th>SATMath</th>\n",
       "      <th>SATWriting</th>\n",
       "      <th>SATVerbal</th>\n",
       "      <th>GPA</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>choice</th>\n",
       "      <th>accepted</th>\n",
       "      <th>school</th>\n",
       "      <th>numapply</th>\n",
       "      <th>habits</th>\n",
       "      <th>SAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>884230</td>\n",
       "      <td>-1.652536</td>\n",
       "      <td>-1.777725</td>\n",
       "      <td>-1.719744</td>\n",
       "      <td>-0.688423</td>\n",
       "      <td>AL</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>4.439333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.918234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>884232</td>\n",
       "      <td>-1.845287</td>\n",
       "      <td>-1.681540</td>\n",
       "      <td>-1.324951</td>\n",
       "      <td>-2.113622</td>\n",
       "      <td>AL</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>4.243038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.809764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>884233</td>\n",
       "      <td>-1.845287</td>\n",
       "      <td>-2.739570</td>\n",
       "      <td>-2.805425</td>\n",
       "      <td>-3.776353</td>\n",
       "      <td>AL</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>4.653213</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.749835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>884247</td>\n",
       "      <td>-0.592404</td>\n",
       "      <td>-0.623510</td>\n",
       "      <td>-0.239270</td>\n",
       "      <td>0.974309</td>\n",
       "      <td>AL</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>4.096910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.544284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>884307</td>\n",
       "      <td>-0.977906</td>\n",
       "      <td>-0.334957</td>\n",
       "      <td>-0.930158</td>\n",
       "      <td>-0.688423</td>\n",
       "      <td>AL</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>4.096910</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.833536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91259</th>\n",
       "      <td>886635</td>\n",
       "      <td>-3.098170</td>\n",
       "      <td>-1.008248</td>\n",
       "      <td>-2.015839</td>\n",
       "      <td>0.261710</td>\n",
       "      <td>FL</td>\n",
       "      <td>Female</td>\n",
       "      <td>Two or more race/ethnicity</td>\n",
       "      <td>5.397940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91260</th>\n",
       "      <td>886640</td>\n",
       "      <td>0.467728</td>\n",
       "      <td>0.530704</td>\n",
       "      <td>0.451618</td>\n",
       "      <td>0.261710</td>\n",
       "      <td>FL</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>4.942008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.540414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91261</th>\n",
       "      <td>886642</td>\n",
       "      <td>-0.110526</td>\n",
       "      <td>1.684918</td>\n",
       "      <td>1.339902</td>\n",
       "      <td>0.974309</td>\n",
       "      <td>FL</td>\n",
       "      <td>Female</td>\n",
       "      <td>Two or more race/ethnicity</td>\n",
       "      <td>4.829304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1.082762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91262</th>\n",
       "      <td>886648</td>\n",
       "      <td>-0.399653</td>\n",
       "      <td>-0.912064</td>\n",
       "      <td>-0.535365</td>\n",
       "      <td>-0.688423</td>\n",
       "      <td>FL</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>4.829304</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.688910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91263</th>\n",
       "      <td>886649</td>\n",
       "      <td>0.853230</td>\n",
       "      <td>-1.296802</td>\n",
       "      <td>-0.732761</td>\n",
       "      <td>0.261710</td>\n",
       "      <td>FL</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>4.096910</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.435814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88662 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SUBJID   SATMath  SATWriting  SATVerbal       GPA state  gender  \\\n",
       "0      884230 -1.652536   -1.777725  -1.719744 -0.688423    AL  Female   \n",
       "1      884232 -1.845287   -1.681540  -1.324951 -2.113622    AL    Male   \n",
       "2      884233 -1.845287   -2.739570  -2.805425 -3.776353    AL    Male   \n",
       "3      884247 -0.592404   -0.623510  -0.239270  0.974309    AL  Female   \n",
       "4      884307 -0.977906   -0.334957  -0.930158 -0.688423    AL    Male   \n",
       "...       ...       ...         ...        ...       ...   ...     ...   \n",
       "91259  886635 -3.098170   -1.008248  -2.015839  0.261710    FL  Female   \n",
       "91260  886640  0.467728    0.530704   0.451618  0.261710    FL  Female   \n",
       "91261  886642 -0.110526    1.684918   1.339902  0.974309    FL  Female   \n",
       "91262  886648 -0.399653   -0.912064  -0.535365 -0.688423    FL    Male   \n",
       "91263  886649  0.853230   -1.296802  -0.732761  0.261710    FL    Male   \n",
       "\n",
       "                             race    income  choice  accepted  school  \\\n",
       "0                           Black  4.439333     4.0       1.0  2192.0   \n",
       "1                           Black  4.243038     1.0       1.0  2192.0   \n",
       "2                           Black  4.653213     4.0       0.0  2192.0   \n",
       "3                           Black  4.096910     1.0       1.0  2192.0   \n",
       "4                           Black  4.096910     4.0       1.0  2192.0   \n",
       "...                           ...       ...     ...       ...     ...   \n",
       "91259  Two or more race/ethnicity  5.397940     1.0       1.0  1691.0   \n",
       "91260                       White  4.942008     1.0       1.0  1691.0   \n",
       "91261  Two or more race/ethnicity  4.829304     1.0       1.0  1691.0   \n",
       "91262                       White  4.829304     2.0       1.0  1691.0   \n",
       "91263                       White  4.096910     3.0       1.0  1691.0   \n",
       "\n",
       "       numapply  habits       SAT  \n",
       "0             9       6 -1.918234  \n",
       "1             6       6 -1.809764  \n",
       "2             4       4 -2.749835  \n",
       "3             9       8 -0.544284  \n",
       "4             9       7 -0.833536  \n",
       "...         ...     ...       ...  \n",
       "91259         6       5 -2.279800  \n",
       "91260         3       7  0.540414  \n",
       "91261         9       6  1.082762  \n",
       "91262         6       7 -0.688910  \n",
       "91263         8       3 -0.435814  \n",
       "\n",
       "[88662 rows x 15 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/02-processed/normalized_data.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total effect of SAT on Acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sat_cols = [\"SAT\", \"GPA\", \"income\"]\n",
    "total_gender_cols = [\"gender\", \"GPA\", \"SAT\", \"race\", \"school\"]\n",
    "outcome = [\"accepted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[total_sat_cols].copy()\n",
    "Y = df[outcome].copy()\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    train_size=10_000,\n",
    "    test_size=10,\n",
    "    random_state=17)\n",
    "\n",
    "# Convert dataframes to np arrays\n",
    "X_train = X_train_df.copy().to_numpy()\n",
    "X_test  = X_test_df.copy().to_numpy()\n",
    "y_train = y_train_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "y_test = y_test_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "\n",
    "# Initialize data dictionary\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "degf = 1\n",
    "data_dict = dict(\n",
    "    n=n,                # num data points\n",
    "    d=d,                # num features\n",
    "    X=X_train,                # data matrix  \n",
    "    y=y_train,                # response variable\n",
    "    p_alpha_df=degf,   # prior deg freedom for alpha\n",
    "    p_alpha_loc=0,      # prior location for alpha\n",
    "    p_alpha_scale=5,  # prior scale for alpha\n",
    "    p_beta_df=degf,    # prior deg freedom for beta\n",
    "    p_beta_loc=0,       # prior location for beta\n",
    "    p_beta_scale=5,    # prior scale for beta\n",
    "    N_new=X_test.shape[0],\n",
    "    X_new=X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter beta has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Warning: The parameter alpha has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/8000)\n",
      "Sampling:   0% (2/8000)\n",
      "Sampling:   0% (3/8000)\n",
      "Sampling:   0% (4/8000)\n",
      "Sampling:   1% (103/8000)\n",
      "Sampling:   3% (202/8000)\n",
      "Sampling:   4% (301/8000)\n",
      "Sampling:   5% (400/8000)\n",
      "Sampling:   6% (500/8000)\n",
      "Sampling:   8% (600/8000)\n",
      "Sampling:   9% (700/8000)\n",
      "Sampling:  10% (800/8000)\n",
      "Sampling:  11% (900/8000)\n",
      "Sampling:  12% (1000/8000)\n",
      "Sampling:  14% (1100/8000)\n",
      "Sampling:  15% (1200/8000)\n",
      "Sampling:  16% (1300/8000)\n",
      "Sampling:  18% (1400/8000)\n",
      "Sampling:  19% (1500/8000)\n",
      "Sampling:  20% (1600/8000)\n",
      "Sampling:  21% (1700/8000)\n",
      "Sampling:  22% (1800/8000)\n",
      "Sampling:  24% (1900/8000)\n",
      "Sampling:  25% (2000/8000)\n",
      "Sampling:  26% (2100/8000)\n",
      "Sampling:  28% (2200/8000)\n",
      "Sampling:  29% (2300/8000)\n",
      "Sampling:  30% (2400/8000)\n",
      "Sampling:  31% (2500/8000)\n",
      "Sampling:  32% (2600/8000)\n",
      "Sampling:  34% (2700/8000)\n",
      "Sampling:  35% (2800/8000)\n",
      "Sampling:  36% (2900/8000)\n",
      "Sampling:  38% (3000/8000)\n",
      "Sampling:  39% (3100/8000)\n",
      "Sampling:  40% (3200/8000)\n",
      "Sampling:  41% (3300/8000)\n",
      "Sampling:  42% (3400/8000)\n",
      "Sampling:  44% (3500/8000)\n",
      "Sampling:  45% (3600/8000)\n",
      "Sampling:  46% (3701/8000)\n",
      "Sampling:  48% (3802/8000)\n",
      "Sampling:  49% (3903/8000)\n",
      "Sampling:  50% (4004/8000)\n",
      "Sampling:  51% (4103/8000)\n",
      "Sampling:  53% (4202/8000)\n",
      "Sampling:  54% (4301/8000)\n",
      "Sampling:  55% (4400/8000)\n",
      "Sampling:  56% (4500/8000)\n",
      "Sampling:  58% (4600/8000)\n",
      "Sampling:  59% (4700/8000)\n",
      "Sampling:  60% (4800/8000)\n",
      "Sampling:  61% (4900/8000)\n",
      "Sampling:  62% (5000/8000)\n",
      "Sampling:  64% (5100/8000)\n",
      "Sampling:  65% (5200/8000)\n",
      "Sampling:  66% (5300/8000)\n",
      "Sampling:  68% (5400/8000)\n",
      "Sampling:  69% (5500/8000)\n",
      "Sampling:  70% (5600/8000)\n",
      "Sampling:  71% (5700/8000)\n",
      "Sampling:  72% (5800/8000)\n",
      "Sampling:  74% (5900/8000)\n",
      "Sampling:  75% (6000/8000)\n",
      "Sampling:  76% (6100/8000)\n",
      "Sampling:  78% (6200/8000)\n",
      "Sampling:  79% (6300/8000)\n",
      "Sampling:  80% (6400/8000)\n",
      "Sampling:  81% (6500/8000)\n",
      "Sampling:  82% (6600/8000)\n",
      "Sampling:  84% (6700/8000)\n",
      "Sampling:  85% (6800/8000)\n",
      "Sampling:  86% (6900/8000)\n",
      "Sampling:  88% (7000/8000)\n",
      "Sampling:  89% (7100/8000)\n",
      "Sampling:  90% (7200/8000)\n",
      "Sampling:  91% (7300/8000)\n",
      "Sampling:  92% (7400/8000)\n",
      "Sampling:  94% (7500/8000)\n",
      "Sampling:  95% (7600/8000)\n",
      "Sampling:  96% (7700/8000)\n",
      "Sampling:  98% (7800/8000)\n",
      "Sampling:  99% (7900/8000)\n",
      "Sampling: 100% (8000/8000)\n",
      "Sampling: 100% (8000/8000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.000489 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 4.89 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.000651 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 6.51 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.000496 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 4.96 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.00047 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 4.7 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    }
   ],
   "source": [
    "fit = run_model(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>1.416</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1.856</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT</th>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPA</th>\n",
       "      <td>0.154</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2484.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1587.0</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "intercept  1.416  0.239   0.939    1.856      0.006    0.004    1588.0   \n",
       "SAT       -0.329  0.026  -0.376   -0.280      0.001    0.000    2540.0   \n",
       "GPA        0.154  0.025   0.109    0.204      0.000    0.000    2484.0   \n",
       "income    -0.120  0.052  -0.218   -0.020      0.001    0.001    1587.0   \n",
       "\n",
       "           ess_tail  r_hat  \n",
       "intercept    1765.0    1.0  \n",
       "SAT          2202.0    1.0  \n",
       "GPA          2388.0    1.0  \n",
       "income       1707.0    1.0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary_table(fit, X_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total effect of SAT scores on acceptance is $\\exp(-0.372) \\approx 0.69 \\approx 31\\%$  decrease in acceptance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct effect of SAT scores on Acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_sat_cols = [\"SAT\", \"gender\", \"race\", \"school\"]\n",
    "dummy_school = pd.get_dummies(df[\"school\"], prefix=\"school\")\n",
    "dummy_race = pd.get_dummies(df[\"race\"], prefix=\"\", prefix_sep=\"\")\n",
    "dummy_gender = pd.get_dummies(df[\"gender\"], prefix=\"\", prefix_sep=\"\")\n",
    "X = pd.concat([df[\"SAT\"], dummy_school, dummy_race, dummy_gender], axis=1).copy()\n",
    "Y = df[outcome].copy()\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    train_size=10_000,\n",
    "    test_size=10,\n",
    "    random_state=17)\n",
    "\n",
    "# Convert dataframes to np arrays\n",
    "X_train = X_train_df.copy().to_numpy()\n",
    "X_test  = X_test_df.copy().to_numpy()\n",
    "y_train = y_train_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "y_test = y_test_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "\n",
    "# Initialize data dictionary\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "degf = 7\n",
    "data_dict = dict(\n",
    "    n=n,                # num data points\n",
    "    d=d,                # num features\n",
    "    X=X_train,                # data matrix  \n",
    "    y=y_train,                # response variable\n",
    "    p_alpha_df=degf,   # prior deg freedom for alpha\n",
    "    p_alpha_loc=0,      # prior location for alpha\n",
    "    p_alpha_scale=5,  # prior scale for alpha\n",
    "    p_beta_df=degf,    # prior deg freedom for beta\n",
    "    p_beta_loc=0,       # prior location for beta\n",
    "    p_beta_scale=5,    # prior scale for beta\n",
    "    N_new=X_test.shape[0],\n",
    "    X_new=X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter beta has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Warning: The parameter alpha has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/8000)\n",
      "Sampling:   0% (2/8000)\n",
      "Sampling:   0% (3/8000)\n",
      "Sampling:   0% (4/8000)\n",
      "Sampling:   1% (103/8000)\n",
      "Sampling:   3% (202/8000)\n",
      "Sampling:   4% (301/8000)\n",
      "Sampling:   5% (400/8000)\n",
      "Sampling:   6% (500/8000)\n",
      "Sampling:   8% (600/8000)\n",
      "Sampling:   9% (700/8000)\n",
      "Sampling:  10% (800/8000)\n",
      "Sampling:  11% (900/8000)\n",
      "Sampling:  12% (1000/8000)\n",
      "Sampling:  14% (1100/8000)\n",
      "Sampling:  15% (1200/8000)\n",
      "Sampling:  16% (1300/8000)\n",
      "Sampling:  18% (1400/8000)\n",
      "Sampling:  19% (1500/8000)\n",
      "Sampling:  20% (1600/8000)\n",
      "Sampling:  21% (1700/8000)\n",
      "Sampling:  22% (1800/8000)\n",
      "Sampling:  24% (1900/8000)\n",
      "Sampling:  25% (2000/8000)\n",
      "Sampling:  26% (2100/8000)\n",
      "Sampling:  28% (2200/8000)\n",
      "Sampling:  29% (2300/8000)\n",
      "Sampling:  30% (2400/8000)\n",
      "Sampling:  31% (2500/8000)\n",
      "Sampling:  32% (2600/8000)\n",
      "Sampling:  34% (2700/8000)\n",
      "Sampling:  35% (2800/8000)\n",
      "Sampling:  36% (2900/8000)\n",
      "Sampling:  38% (3000/8000)\n",
      "Sampling:  39% (3100/8000)\n",
      "Sampling:  40% (3200/8000)\n",
      "Sampling:  41% (3300/8000)\n",
      "Sampling:  42% (3400/8000)\n",
      "Sampling:  44% (3500/8000)\n",
      "Sampling:  45% (3600/8000)\n",
      "Sampling:  45% (3601/8000)\n",
      "Sampling:  46% (3701/8000)\n",
      "Sampling:  46% (3702/8000)\n",
      "Sampling:  48% (3802/8000)\n",
      "Sampling:  49% (3902/8000)\n",
      "Sampling:  49% (3903/8000)\n",
      "Sampling:  50% (4002/8000)\n",
      "Sampling:  51% (4101/8000)\n",
      "Sampling:  53% (4201/8000)\n",
      "Sampling:  53% (4202/8000)\n",
      "Sampling:  54% (4301/8000)\n",
      "Sampling:  55% (4401/8000)\n",
      "Sampling:  56% (4501/8000)\n",
      "Sampling:  58% (4601/8000)\n",
      "Sampling:  59% (4701/8000)\n",
      "Sampling:  60% (4800/8000)\n",
      "Sampling:  61% (4900/8000)\n",
      "Sampling:  62% (5000/8000)\n",
      "Sampling:  64% (5100/8000)\n",
      "Sampling:  65% (5200/8000)\n",
      "Sampling:  66% (5300/8000)\n",
      "Sampling:  68% (5400/8000)\n",
      "Sampling:  69% (5500/8000)\n",
      "Sampling:  70% (5600/8000)\n",
      "Sampling:  71% (5700/8000)\n",
      "Sampling:  72% (5800/8000)\n",
      "Sampling:  74% (5900/8000)\n",
      "Sampling:  75% (6000/8000)\n",
      "Sampling:  76% (6100/8000)\n",
      "Sampling:  78% (6200/8000)\n",
      "Sampling:  79% (6300/8000)\n",
      "Sampling:  80% (6400/8000)\n",
      "Sampling:  81% (6500/8000)\n",
      "Sampling:  82% (6600/8000)\n",
      "Sampling:  84% (6700/8000)\n",
      "Sampling:  85% (6800/8000)\n",
      "Sampling:  86% (6900/8000)\n",
      "Sampling:  88% (7000/8000)\n",
      "Sampling:  89% (7100/8000)\n",
      "Sampling:  90% (7200/8000)\n",
      "Sampling:  91% (7300/8000)\n",
      "Sampling:  92% (7400/8000)\n",
      "Sampling:  94% (7500/8000)\n",
      "Sampling:  95% (7600/8000)\n",
      "Sampling:  96% (7700/8000)\n",
      "Sampling:  98% (7800/8000)\n",
      "Sampling:  99% (7900/8000)\n",
      "Sampling: 100% (8000/8000)\n",
      "Sampling: 100% (8000/8000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.002508 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 25.08 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.003686 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 36.86 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.005073 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 50.73 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.006351 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 63.51 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    }
   ],
   "source": [
    "fit = run_model(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SAT</th>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2668.0</td>\n",
       "      <td>3062.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
       "SAT -0.31  0.037  -0.381   -0.242      0.001    0.001    2668.0    3062.0   \n",
       "\n",
       "     r_hat  \n",
       "SAT    1.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary_table(fit, X_train_df, [\"SAT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total effect of Gender/Race on Acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df[\"SAT\"], dummy_gender, dummy_race, dummy_school], axis=1)\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    train_size=10_000,\n",
    "    test_size=10,\n",
    "    random_state=17)\n",
    "\n",
    "# Convert dataframes to np arrays\n",
    "X_train = X_train_df.copy().to_numpy()\n",
    "X_test  = X_test_df.copy().to_numpy()\n",
    "y_train = y_train_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "y_test = y_test_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "\n",
    "# Initialize data dictionary\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "degf = 7\n",
    "data_dict = dict(\n",
    "    n=n,                # num data points\n",
    "    d=d,                # num features\n",
    "    X=X_train,                # data matrix  \n",
    "    y=y_train,                # response variable\n",
    "    p_alpha_df=degf,   # prior deg freedom for alpha\n",
    "    p_alpha_loc=0,      # prior location for alpha\n",
    "    p_alpha_scale=5,  # prior scale for alpha\n",
    "    p_beta_df=degf,    # prior deg freedom for beta\n",
    "    p_beta_loc=0,       # prior location for beta\n",
    "    p_beta_scale=5,    # prior scale for beta\n",
    "    N_new=X_test.shape[0],\n",
    "    X_new=X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fit = run_model(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.609</td>\n",
       "      <td>2.993</td>\n",
       "      <td>-4.991</td>\n",
       "      <td>6.294</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.404</td>\n",
       "      <td>2.992</td>\n",
       "      <td>-5.187</td>\n",
       "      <td>6.065</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>1703.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Indian</th>\n",
       "      <td>1.281</td>\n",
       "      <td>2.112</td>\n",
       "      <td>-2.947</td>\n",
       "      <td>5.196</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.056</td>\n",
       "      <td>711.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>-0.704</td>\n",
       "      <td>1.795</td>\n",
       "      <td>-4.130</td>\n",
       "      <td>2.578</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>565.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black</th>\n",
       "      <td>-0.084</td>\n",
       "      <td>1.792</td>\n",
       "      <td>-3.635</td>\n",
       "      <td>3.056</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>568.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>0.041</td>\n",
       "      <td>1.793</td>\n",
       "      <td>-3.455</td>\n",
       "      <td>3.260</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>566.0</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>-0.175</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-3.668</td>\n",
       "      <td>3.078</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>568.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two or more race/ethnicity</th>\n",
       "      <td>0.086</td>\n",
       "      <td>1.795</td>\n",
       "      <td>-3.331</td>\n",
       "      <td>3.374</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>565.0</td>\n",
       "      <td>969.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>0.356</td>\n",
       "      <td>1.793</td>\n",
       "      <td>-3.060</td>\n",
       "      <td>3.649</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>566.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
       "Female                      0.609  2.993  -4.991    6.294      0.088    0.062   \n",
       "Male                        0.404  2.992  -5.187    6.065      0.088    0.062   \n",
       "American Indian             1.281  2.112  -2.947    5.196      0.079    0.056   \n",
       "Asian                      -0.704  1.795  -4.130    2.578      0.075    0.053   \n",
       "Black                      -0.084  1.792  -3.635    3.056      0.075    0.053   \n",
       "Hispanic                    0.041  1.793  -3.455    3.260      0.075    0.053   \n",
       "Other                      -0.175  1.800  -3.668    3.078      0.075    0.053   \n",
       "Two or more race/ethnicity  0.086  1.795  -3.331    3.374      0.075    0.053   \n",
       "White                       0.356  1.793  -3.060    3.649      0.075    0.053   \n",
       "\n",
       "                            ess_bulk  ess_tail  r_hat  \n",
       "Female                        1153.0    1752.0   1.00  \n",
       "Male                          1153.0    1703.0   1.00  \n",
       "American Indian                711.0     995.0   1.01  \n",
       "Asian                          565.0     932.0   1.01  \n",
       "Black                          568.0    1009.0   1.01  \n",
       "Hispanic                       566.0    1008.0   1.01  \n",
       "Other                          568.0     944.0   1.01  \n",
       "Two or more race/ethnicity     565.0     969.0   1.01  \n",
       "White                          566.0     944.0   1.01  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary_table(fit, X_train_df, list(dummy_gender) + list(dummy_race))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both gender and race have a positive total effect on acceptance with the exception of being Asian and Hispanic. \n",
    "These correspond to a $\\exp(-0.718) \\approx 0.49 \\approx 51\\%$ and $\\exp(-0.307) \\approx 0.74 \\approx 26\\%$ reduction in the odds of getting accepted into their first choice school.\n",
    "However the hdi for all variables includes negative and positive values and so the mean being centered between 0 to 1 for all variables could indicate a low degree of confidence for these estimated values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct effect of Race on Accpetance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dummy_race\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    train_size=10_000,\n",
    "    test_size=10,\n",
    "    random_state=17)\n",
    "\n",
    "# Convert dataframes to np arrays\n",
    "X_train = X_train_df.copy().to_numpy()\n",
    "X_test  = X_test_df.copy().to_numpy()\n",
    "y_train = y_train_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "y_test = y_test_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "\n",
    "# Initialize data dictionary\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "degf = 7\n",
    "data_dict = dict(\n",
    "    n=n,                # num data points\n",
    "    d=d,                # num features\n",
    "    X=X_train,                # data matrix  \n",
    "    y=y_train,                # response variable\n",
    "    p_alpha_df=degf,   # prior deg freedom for alpha\n",
    "    p_alpha_loc=0,      # prior location for alpha\n",
    "    p_alpha_scale=5,  # prior scale for alpha\n",
    "    p_beta_df=degf,    # prior deg freedom for beta\n",
    "    p_beta_loc=0,       # prior location for beta\n",
    "    p_beta_scale=5,    # prior scale for beta\n",
    "    N_new=X_test.shape[0],\n",
    "    X_new=X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fit = run_model(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.808</td>\n",
       "      <td>1.738</td>\n",
       "      <td>-2.339</td>\n",
       "      <td>4.346</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.087</td>\n",
       "      <td>331.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Indian</th>\n",
       "      <td>1.520</td>\n",
       "      <td>2.031</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>5.326</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.074</td>\n",
       "      <td>383.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>-1.039</td>\n",
       "      <td>1.738</td>\n",
       "      <td>-4.566</td>\n",
       "      <td>2.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.087</td>\n",
       "      <td>331.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black</th>\n",
       "      <td>0.094</td>\n",
       "      <td>1.738</td>\n",
       "      <td>-3.686</td>\n",
       "      <td>3.006</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.083</td>\n",
       "      <td>332.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>-0.142</td>\n",
       "      <td>1.739</td>\n",
       "      <td>-4.054</td>\n",
       "      <td>2.649</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.086</td>\n",
       "      <td>332.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>-0.370</td>\n",
       "      <td>1.745</td>\n",
       "      <td>-3.920</td>\n",
       "      <td>2.651</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.087</td>\n",
       "      <td>332.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two or more race/ethnicity</th>\n",
       "      <td>0.006</td>\n",
       "      <td>1.740</td>\n",
       "      <td>-3.543</td>\n",
       "      <td>3.149</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.085</td>\n",
       "      <td>332.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>0.337</td>\n",
       "      <td>1.738</td>\n",
       "      <td>-3.158</td>\n",
       "      <td>3.497</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.080</td>\n",
       "      <td>332.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
       "intercept                   0.808  1.738  -2.339    4.346      0.097    0.087   \n",
       "American Indian             1.520  2.031  -2.514    5.326      0.105    0.074   \n",
       "Asian                      -1.039  1.738  -4.566    2.097      0.097    0.087   \n",
       "Black                       0.094  1.738  -3.686    3.006      0.097    0.083   \n",
       "Hispanic                   -0.142  1.739  -4.054    2.649      0.097    0.086   \n",
       "Other                      -0.370  1.745  -3.920    2.651      0.098    0.087   \n",
       "Two or more race/ethnicity  0.006  1.740  -3.543    3.149      0.097    0.085   \n",
       "White                       0.337  1.738  -3.158    3.497      0.097    0.080   \n",
       "\n",
       "                            ess_bulk  ess_tail  r_hat  \n",
       "intercept                      331.0     291.0   1.02  \n",
       "American Indian                383.0     527.0   1.01  \n",
       "Asian                          331.0     293.0   1.02  \n",
       "Black                          332.0     299.0   1.02  \n",
       "Hispanic                       332.0     286.0   1.02  \n",
       "Other                          332.0     282.0   1.02  \n",
       "Two or more race/ethnicity     332.0     290.0   1.02  \n",
       "White                          332.0     289.0   1.02  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary_table(fit, X_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The direct effect of being Asian corresponds to a 56\\% reduction in the odds of being accepted. \n",
    "Being hispanic now corresponds to only a $.002%$ reduction in the odds of being accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct effect of Gender on Acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dummy_gender\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    train_size=10_000,\n",
    "    test_size=10,\n",
    "    random_state=17)\n",
    "\n",
    "# Convert dataframes to np arrays\n",
    "X_train = X_train_df.copy().to_numpy()\n",
    "X_test  = X_test_df.copy().to_numpy()\n",
    "y_train = y_train_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "y_test = y_test_df.copy().astype(np.int8).to_numpy().flatten()\n",
    "\n",
    "# Initialize data dictionary\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "degf = 7\n",
    "data_dict = dict(\n",
    "    n=n,                # num data points\n",
    "    d=d,                # num features\n",
    "    X=X_train,                # data matrix  \n",
    "    y=y_train,                # response variable\n",
    "    p_alpha_df=degf,   # prior deg freedom for alpha\n",
    "    p_alpha_loc=0,      # prior location for alpha\n",
    "    p_alpha_scale=5,  # prior scale for alpha\n",
    "    p_beta_df=degf,    # prior deg freedom for beta\n",
    "    p_beta_loc=0,       # prior location for beta\n",
    "    p_beta_scale=5,    # prior scale for beta\n",
    "    N_new=X_test.shape[0],\n",
    "    X_new=X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fit = run_model(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.443</td>\n",
       "      <td>2.946</td>\n",
       "      <td>-4.773</td>\n",
       "      <td>6.886</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.132</td>\n",
       "      <td>437.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.468</td>\n",
       "      <td>2.946</td>\n",
       "      <td>-5.302</td>\n",
       "      <td>6.384</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.140</td>\n",
       "      <td>436.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.332</td>\n",
       "      <td>2.946</td>\n",
       "      <td>-6.088</td>\n",
       "      <td>5.555</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.140</td>\n",
       "      <td>436.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "intercept  0.443  2.946  -4.773    6.886      0.146    0.132     437.0   \n",
       "Female     0.468  2.946  -5.302    6.384      0.146    0.140     436.0   \n",
       "Male       0.332  2.946  -6.088    5.555      0.146    0.140     436.0   \n",
       "\n",
       "           ess_tail  r_hat  \n",
       "intercept     120.0   1.03  \n",
       "Female        123.0   1.03  \n",
       "Male          126.0   1.03  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary_table(fit, X_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both male and females have a po"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bayes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c627b44f4ae38c891954d455ae816164b6db6e9971d5361176b0256385e6703d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
