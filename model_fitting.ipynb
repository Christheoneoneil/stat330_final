{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christheoneoneil/stat330_final/blob/model_fitting/model_fitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b06MBAFmSIfx"
      },
      "source": [
        "#Relevant Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYc-479GPZAa",
        "outputId": "717a082b-bfb4-4eed-86ba-9b26140913a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nest_asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: nest-asyncio\n",
            "Successfully installed nest-asyncio-1.5.6\n"
          ]
        }
      ],
      "source": [
        "import stan\n",
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import arviz as az\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminary Code Cleaning for Multinomial Logistic Regression"
      ],
      "metadata": {
        "id": "k_b-TZPNUgDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_name: str):\n",
        "  \"\"\"\n",
        "  param filename: name of data file\n",
        "  returns: pandas data frame\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(file_name, index_col = \"Unnamed: 0\")\n",
        "  df.dropna(axis = 0, how=\"any\", inplace=True)\n",
        "  df = df.loc[:,~df.columns.duplicated()]\n",
        "  return df"
      ],
      "metadata": {
        "id": "nIwv-ZxRU47Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data(df: pd.DataFrame, key: str, unwanted_cols: list):\n",
        "  \"\"\"\n",
        "  param df: data frame that needs to be formatted\n",
        "  key: key value associated with provided dictionary\n",
        "  unwanted_cols: columns that are ultimatly not needed for analysis\n",
        "  returns: prept data frame for logit regression\n",
        "  \"\"\"\n",
        "  from sklearn import preprocessing\n",
        "  df_copy = df.copy()\n",
        "  unnormed_cols = [\"STUDWGT\"]\n",
        "\n",
        "  if key == \"imputed\":\n",
        "    unnormed_cols = ['ACTComposite', 'SATMath', \n",
        "                     'SATVerbal', 'SATWriting'] + unnormed_cols\n",
        "    df_copy.drop(columns=[\"Unnamed: 0.1\"], inplace=True)\n",
        "\n",
        "  normalizer = preprocessing.MinMaxScaler()\n",
        "  normed_cols = normalizer.fit_transform(df[unnormed_cols])\n",
        "  df_copy[unnormed_cols] = normed_cols\n",
        "  df_copy.drop(columns=unwanted_cols, inplace=True)\n",
        "\n",
        "  recode_vars = [\"STRAT\", \"SELECTIVITY\", \"DOBYear\"]\n",
        "  for var in recode_vars:\n",
        "    unencoded_list = list(df_copy[var].unique())\n",
        "    encode_list = list(range(1, len(unencoded_list)+1))\n",
        "    df_copy[var].replace(unencoded_list, encode_list, inplace=True)\n",
        "  return(df_copy)"
      ],
      "metadata": {
        "id": "T2N019nSWTZ7"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {\"imputed\": read_data(\"final_frame_imputed.csv\"),\n",
        "           \"non_imputed\": read_data(\"final_frame_non_imputed.csv\")}\n",
        "unnecessary_cols = [\"ACERECODE\"]\n",
        "for key, df in df_dict.items():\n",
        "  df_dict[key] = prep_data(df, key, unnecessary_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD69j_rfUfm8",
        "outputId": "b60c6877-3c9a-4620-8192-509ca2f68dc9"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
            " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29.]\n",
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
            " 19. 20. 21. 22. 23. 24. 25. 26. 27.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvRIiikxSlhT"
      },
      "source": [
        "#Setting Up Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_zUYXChSbJZ"
      },
      "outputs": [],
      "source": [
        "def fit_model(model_code: str, X: np.array, Y: np.array, n: int, k: int, \n",
        "              flag_val: int):\n",
        "  \"\"\"\n",
        "  param model_code: stan formatted code for model\n",
        "  param X: nxk array of covariates \n",
        "  param Y: array of target values\n",
        "  param n: number of rows \n",
        "  param k: number of covarites\n",
        "  param flag_val: do predictive distrub flag value\n",
        "  reutrns: stan sampler object\n",
        "  \"\"\"\n",
        "  mod = stan.build(model_code, data={\"X\": X, \"Y\": Y, \"n\": n, \"k\": k, \n",
        "                               \"do_prior_predictive\": flag_val})\n",
        "  return mod.sample()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-lYrQWBUgwq"
      },
      "outputs": [],
      "source": [
        "multinomial_log_stan_code = \"\"\"\n",
        "data {\n",
        "  int<lower=0> n;                // number of units\n",
        "  int<lower=0> k;                // number of covariates\n",
        "  matrix[n, k] X;            // covariates for each entry, including the intercept covariate\n",
        "  int<lower=1,upper=5> Y[n];     // categorical\n",
        "\n",
        "  int do_prior_predictive;\n",
        "}\n",
        "parameters {\n",
        "  vector[k] beta;            // the coefficients\n",
        "}\n",
        "transformed parameters {\n",
        "  vector[n] eta;                // linear predictors\n",
        "  eta = X * beta;\n",
        "}\n",
        "model {\n",
        "  beta ~ normal(0, 3);\n",
        "\n",
        "  if (do_prior_predictive != 1) {\n",
        "    for (i in 1:n)\n",
        "      Y[i] ~ categorical_logit(eta);\n",
        "  }\n",
        "}\n",
        "generated quantities {\n",
        "  int<lower=1,upper=5> Y_tilde[n];\n",
        "  for (i in 1:n)  \n",
        "    Y_tilde[i] = categorical_logit_rng(eta);  \n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FICIRLDwVXAL",
        "outputId": "8767c917-11ec-4c24-f0de-a474ef2b203c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Building: 43.4s, done.Sampling:   0%\n",
            "Sampling:   2% (200/8000)\n",
            "Sampling:   4% (300/8000)\n",
            "Sampling:   5% (400/8000)\n",
            "Sampling:   6% (500/8000)\n",
            "Sampling:   8% (600/8000)\n",
            "Sampling:   9% (700/8000)\n",
            "Sampling:  10% (800/8000)\n",
            "Sampling:  11% (900/8000)\n",
            "Sampling:  12% (1000/8000)\n",
            "Sampling:  14% (1100/8000)\n",
            "Sampling:  15% (1200/8000)\n",
            "Sampling:  16% (1300/8000)\n",
            "Sampling:  18% (1400/8000)\n",
            "Sampling:  19% (1500/8000)\n",
            "Sampling:  20% (1600/8000)\n",
            "Sampling:  21% (1701/8000)\n",
            "Sampling:  23% (1801/8000)\n",
            "Sampling:  24% (1901/8000)\n",
            "Sampling:  25% (2002/8000)\n",
            "Sampling:  26% (2101/8000)\n",
            "Sampling:  28% (2200/8000)\n",
            "Sampling:  29% (2300/8000)\n",
            "Sampling:  30% (2400/8000)\n",
            "Sampling:  31% (2500/8000)\n",
            "Sampling:  32% (2600/8000)\n",
            "Sampling:  34% (2700/8000)\n",
            "Sampling:  35% (2800/8000)\n",
            "Sampling:  36% (2900/8000)\n",
            "Sampling:  38% (3000/8000)\n",
            "Sampling:  39% (3100/8000)\n",
            "Sampling:  40% (3200/8000)\n",
            "Sampling:  41% (3300/8000)\n",
            "Sampling:  42% (3400/8000)\n",
            "Sampling:  44% (3500/8000)\n",
            "Sampling:  45% (3600/8000)\n",
            "Sampling:  46% (3700/8000)\n",
            "Sampling:  48% (3800/8000)\n",
            "Sampling:  49% (3900/8000)\n",
            "Sampling:  49% (3901/8000)\n",
            "Sampling:  50% (4000/8000)\n",
            "Sampling:  51% (4100/8000)\n",
            "Sampling:  52% (4200/8000)\n",
            "Sampling:  54% (4300/8000)\n",
            "Sampling:  55% (4400/8000)\n",
            "Sampling:  56% (4500/8000)\n",
            "Sampling:  56% (4501/8000)\n",
            "Sampling:  58% (4601/8000)\n",
            "Sampling:  59% (4701/8000)\n",
            "Sampling:  60% (4800/8000)\n",
            "Sampling:  61% (4900/8000)\n",
            "Sampling:  62% (5000/8000)\n",
            "Sampling:  64% (5100/8000)\n",
            "Sampling:  65% (5200/8000)\n",
            "Sampling:  66% (5301/8000)\n",
            "Sampling:  68% (5401/8000)\n",
            "Sampling:  69% (5501/8000)\n",
            "Sampling:  70% (5601/8000)\n",
            "Sampling:  71% (5701/8000)\n",
            "Sampling:  73% (5801/8000)\n",
            "Sampling:  74% (5901/8000)\n",
            "Sampling:  75% (6002/8000)\n",
            "Sampling:  76% (6101/8000)\n",
            "Sampling:  78% (6200/8000)\n",
            "Sampling:  79% (6300/8000)\n",
            "Sampling:  80% (6400/8000)\n",
            "Sampling:  81% (6500/8000)\n",
            "Sampling:  82% (6600/8000)\n",
            "Sampling:  84% (6700/8000)\n",
            "Sampling:  85% (6800/8000)\n",
            "Sampling:  86% (6900/8000)\n",
            "Sampling:  88% (7000/8000)"
          ]
        }
      ],
      "source": [
        "target_col = \"CHOICE\"\n",
        "prior_pred_dict = {}\n",
        "for key, df in df_dict.items():\n",
        "  covars = [cov for cov in list(df.columns) if cov not in target_col]\n",
        "  X = np.array(df[covars])\n",
        "  n = len(X)\n",
        "  X = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
        "  Y = np.array(df[target_col], dtype=\"int64\")\n",
        "  k = X.shape[1]\n",
        "  #prior_pred_dict[key] = fit_model(multinomial_log_stan_code, X, Y, n, k, 1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNOF+StbnGmPh8dwOYDRk5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}